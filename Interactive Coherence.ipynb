{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.coherence_models import BigramCoherence\n",
    "from utils.lm_utils import Corpus, SentCorpus\n",
    "import argparse\n",
    "import config\n",
    "from add_args import add_bigram_args\n",
    "from utils.data_utils import DataSet\n",
    "from preprocess import get_lm_hidden\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40812, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.52it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 2048\n",
    "parser = argparse.ArgumentParser()\n",
    "add_bigram_args(parser)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "dataset = DataSet(config.DATASET[args.data_name])\n",
    "#dataset.data_path = \"../\" + dataset.data_path\n",
    "\n",
    "train_dataset = dataset.load_train(args.portion)\n",
    "test_dataset = dataset.load_test(args.portion)\n",
    "\n",
    "corpus = Corpus(train_dataset.file_list, test_dataset.file_list, config.WORD_EMBEDDING)\n",
    "sent_embedding = get_lm_hidden(args.data_name, \"lm_\" + args.data_name, corpus, limit=100)\n",
    "\n",
    "\n",
    "    \n",
    "kwargs = {\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"sent_encoder\": sent_embedding,\n",
    "    \"hparams\": {\n",
    "        \"loss\": args.loss,\n",
    "        \"input_dropout\": args.input_dropout,\n",
    "        \"hidden_state\": args.hidden_state,\n",
    "        \"hidden_layers\": args.hidden_layers,\n",
    "        \"hidden_dropout\": args.hidden_dropout,\n",
    "        \"num_epochs\": args.num_epochs,\n",
    "        \"margin\": args.margin,\n",
    "        \"lr\": args.lr,\n",
    "        \"l2_reg_lambda\": args.l2_reg_lambda,\n",
    "        \"use_bn\": args.use_bn,\n",
    "        \"task\": \"discrimination\",\n",
    "        \"bidirectional\": args.bidirectional,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramCoherence(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniter/anaconda3/envs/coh/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/daniter/anaconda3/envs/coh/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/daniter/anaconda3/envs/coh/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/daniter/anaconda3/envs/coh/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./checkpoint/wsj_bigram-0.9379.pth\", map_location=torch.device('cpu'))\n",
    "model.use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.load_test(args.portion)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "test_df = dataset.load_test_perm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos score stats\n",
      "                  0\n",
      "count  23476.000000\n",
      "mean       0.125422\n",
      "std        3.443641\n",
      "min      -26.557003\n",
      "25%       -1.327920\n",
      "50%        0.366005\n",
      "75%        1.852593\n",
      "max       17.844749\n",
      "neg score stats\n",
      "                   0\n",
      "count  469520.000000\n",
      "mean        1.605783\n",
      "std         3.498630\n",
      "min       -26.557003\n",
      "25%        -0.318012\n",
      "50%         1.167307\n",
      "75%         3.043002\n",
      "max        24.444691\n"
     ]
    }
   ],
   "source": [
    "test = test_dataloader\n",
    "df = test_df\n",
    "debug = True\n",
    "correct_pred = [0, 0, 0]\n",
    "total_samples = [0, 0, 0]\n",
    "\n",
    "if debug:\n",
    "    all_pos_scores = []\n",
    "    all_neg_scores = []\n",
    "\n",
    "model.discriminator.train(False)\n",
    "i = 0\n",
    "for article in test:\n",
    "    i += 1\n",
    "    if i == 2:\n",
    "        break\n",
    "    sentences = df.loc[article[0], \"sentences\"].split(\"<PUNC>\")\n",
    "    sent_num = len(sentences)\n",
    "    sentences = [\"<SOA>\"] + sentences + [\"<EOA>\"]\n",
    "    neg_sentences_list = df.loc[article[0],\n",
    "                                \"neg_list\"].split(\"<BREAK>\")\n",
    "    neg_sentences_list = [s.split(\"<PUNC>\")\n",
    "                          for s in neg_sentences_list]\n",
    "\n",
    "    pos_sent1 = sentences[:-1]\n",
    "    pos_sent1 = model.encode(pos_sent1)\n",
    "    pos_sent2 = sentences[1:]\n",
    "    pos_sent2 = model.encode(pos_sent2)\n",
    "    pos_scores = model.discriminator(pos_sent1, pos_sent2)\n",
    "    mean_pos_score = pos_scores.mean().data.cpu().numpy()\n",
    "\n",
    "    if debug:\n",
    "        all_pos_scores.append(pos_scores.data.cpu().numpy().squeeze())\n",
    "\n",
    "    mean_neg_scores = []\n",
    "    for neg_sentences in neg_sentences_list:\n",
    "        neg_sentences = [\"<SOA>\"] + neg_sentences + [\"<EOA>\"]\n",
    "        neg_sent1 = neg_sentences[:-1]\n",
    "        neg_sent1 = model.encode(neg_sent1)\n",
    "        neg_sent2 = neg_sentences[1:]\n",
    "        neg_sent2 = model.encode(neg_sent2)\n",
    "        neg_scores = model.discriminator(neg_sent1, neg_sent2)\n",
    "        mean_neg_score = neg_scores.mean().data.cpu().numpy()\n",
    "        mean_neg_scores.append(mean_neg_score)\n",
    "\n",
    "        if debug:\n",
    "            all_neg_scores.append(\n",
    "                neg_scores.data.cpu().numpy().squeeze())\n",
    "\n",
    "    for mean_neg_score in mean_neg_scores:\n",
    "        if mean_pos_score < mean_neg_score:\n",
    "            for i in range(3):\n",
    "                lower_bound = model.intervals[i]\n",
    "                upper_bound = model.intervals[i + 1]\n",
    "                if (sent_num > lower_bound) and (sent_num <= upper_bound):\n",
    "                    correct_pred[i] += 1\n",
    "        for i in range(3):\n",
    "            lower_bound = model.intervals[i]\n",
    "            upper_bound = model.intervals[i + 1]\n",
    "            if (sent_num > lower_bound) and (sent_num <= upper_bound):\n",
    "                total_samples[i] += 1\n",
    "# model.discriminator.train(True)\n",
    "accs = np.true_divide(correct_pred, total_samples)\n",
    "acc = np.true_divide(np.sum(correct_pred), np.sum(total_samples))\n",
    "\n",
    "if debug:\n",
    "    all_pos_scores = np.concatenate(all_pos_scores)\n",
    "    all_neg_scores = np.concatenate(all_neg_scores)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    print('pos score stats')\n",
    "    print(pd.DataFrame(all_pos_scores).describe())\n",
    "\n",
    "    print('neg score stats')\n",
    "    print(pd.DataFrame(all_neg_scores).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417457305502847"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.loc[\"./data/parsed_wsj/20/wsj_2052\", \"sentences\"].split(\"<PUNC>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the Soviets announced their last soldier had left Afghanistan in February , the voices of skepticism were all but drowned out by an international chorus of euphoria .\n",
      "President Chen Shui - bian visited the Nicaraguan National Assembly on August 17 , where he received a medal from the president of the assembly , Ivan Escobar Fornos .\n"
     ]
    }
   ],
   "source": [
    "print(s[0])\n",
    "print(s1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-1a51829b98e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#s1 = s[:-1][:1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#s2 = s[1:][:1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpos_sent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpos_sent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_sent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_sent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jurafsky/srl-coh/cross_domain_coherence/models/coherence_models.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_pretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         sentences, lengths, idx_sort = self.sent_encoder.prepare_samples(\n",
      "\u001b[0;32m~/Documents/jurafsky/srl-coh/cross_domain_coherence/models/coherence_models.py\u001b[0m in \u001b[0;36m_variable\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "#s1 = s[:-1][:1]\n",
    "#s2 = s[1:][:1]\n",
    "pos_sent1 = model.encode(s1)\n",
    "pos_sent2 = model.encode(s2)\n",
    "model.discriminator(pos_sent1, pos_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2381e-01,  2.9231e-01,  8.7722e-01,  ...,  2.4608e-01,\n",
       "         -4.1085e-05,  7.4488e-01]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.6557139e-01,  7.4483222e-01,  8.2124192e-01, ...,\n",
       "        7.8131747e-01, -4.4397300e-05,  8.9844376e-01], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.sent_encoder.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from models.language_models import LanguageModel\n",
    "\n",
    "def one_the_fly_encoder(sentences, lm_name=\"lm_\" + args.data_name, corpus=corpus):\n",
    "    with open(os.path.join(config.CHECKPOINT_PATH, lm_name + \"_forward.pkl\"), \"rb\") as f:\n",
    "        hparams = pickle.load(f)\n",
    "\n",
    "    kwargs = {\n",
    "        \"vocab_size\": corpus.glove_embed.shape[0],\n",
    "        \"embed_dim\": corpus.glove_embed.shape[1],\n",
    "        \"corpus\": corpus,\n",
    "        \"hparams\": hparams,\n",
    "    }\n",
    "\n",
    "    forward_lm = LanguageModel(**kwargs)\n",
    "    forward_lm.load(os.path.join(config.CHECKPOINT_PATH, lm_name + \"_forward.pt\"))\n",
    "    forward_lm = forward_lm.lm\n",
    "    forward_lm.eval()\n",
    "\n",
    "    backward_lm = LanguageModel(**kwargs)\n",
    "    backward_lm.load(os.path.join(config.CHECKPOINT_PATH, lm_name + \"_backward.pt\"))\n",
    "    backward_lm = backward_lm.lm\n",
    "    backward_lm.eval()\n",
    "\n",
    "    embed_dict = {}\n",
    "    ini_hidden = forward_lm.init_hidden(1)\n",
    "    for sent in sentences:\n",
    "        fs = [corpus.vocab[w] for w in ['<eos>'] + sent.split() + ['<eos>']]\n",
    "        fs = torch.LongTensor(fs).unsqueeze(1)\n",
    "        if torch.cuda.is_available():\n",
    "            fs = fs.to('cuda')\n",
    "        fout = forward_lm.encode(fs, ini_hidden)\n",
    "        fout = torch.max(fout, 0)[0].squeeze().data.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        bs = [corpus.vocab[w] for w in ['<eos>'] + sent.split()[::-1] + ['<eos>']]\n",
    "        bs = torch.LongTensor(bs).unsqueeze(1)\n",
    "        if torch.cuda.is_available():\n",
    "            bs = bs.to('cuda')\n",
    "        bout = backward_lm.encode(bs, ini_hidden)\n",
    "        bout = torch.max(bout, 0)[0].squeeze().data.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        embed_dict[sent] = np.hstack((fout, bout))\n",
    "    np.random.seed(0)\n",
    "    embed_dict[\"<SOA>\"] = np.random.uniform(size=2048).astype(np.float32)\n",
    "    embed_dict[\"<EOA>\"] = np.random.uniform(size=2048).astype(np.float32)\n",
    "    return embed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6085], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = [\"President Chen Shui - bian visited the Nicaraguan National Assembly on August 17 , where he received a medal from the president of the assembly , Ivan Escobar Fornos .\"]\n",
    "s2 = [\"On August 25 President Chen Shui - bian wrapped up his first overseas trip since taking office , swinging through three countries in Latin America and another three in Africa .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "model.discriminator(pos_sent1, pos_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7936], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = [\"President Chen Shui - bian visited the Nicaraguan National Assembly on August 17 , where he received a medal from the president of the assembly , Ivan Escobar Fornos .\"]\n",
    "s2 = [\"On August 25 Ivan Escobar Fornos wrapped up his first overseas trip since taking office , swinging through three countries in Latin America and another three in Africa .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "model.discriminator(pos_sent1, pos_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1014], grad_fn=<DivBackward0>)\n",
      "tensor([3.5262], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = [\"Chen Shui - bian left on August 13 for a 13 - day trip that included state visits to the Latin American nations of the Dominican Republic , Nicaragua , Costa Rica , as well as the west African nations of Gambia , Burkina Faso , and Chad .\"]\n",
    "s2 = [\"Before boarding the plane on the 13th , Chen Shui - bian delivered a short speech in which he stated , \\\" Taiwan must stand up and be counted . \\\"\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n",
    "s1 = [\"Chen Shui - bian left on August 13 for a 13 - day trip that included state visits to the Latin American nations of the Dominican Republic , Nicaragua , Costa Rica , as well as the west African nations of Gambia , Burkina Faso , and Chad .\"]\n",
    "s2 = [\"Before boarding the plane on the 13th , Burkina Faso delivered a short speech in which he stated , \\\" Taiwan must stand up and be counted . \\\"\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5484], grad_fn=<DivBackward0>)\n",
      "tensor([-0.3501], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = [\"He attended the presidential inauguration there on the 16th , and was able to meet with the leaders and representatives of friendly nations from around the world .\"]\n",
    "s2 = [\"On the 17th he continued on to Nicaragua , where he met with President Arnoldo Aleman and delivered a speech to the Nicaraguan National Assembly .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n",
    "s1 = [\"He attended the presidential inauguration there on the 16th , and was able to meet with the leaders and representatives of friendly nations from around the world .\"]\n",
    "s2 = [\"On the 17th leaders and representatives continued on to Nicaragua , where he met with President Arnoldo Aleman and delivered a speech to the Nicaraguan National Assembly .\"]\n",
    "\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6480], grad_fn=<DivBackward0>)\n",
      "tensor([-1.1668], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s1 = [\"The next day , Chen took advantage of the upcoming presidential inauguration to meet with leaders from a number of countries with which the ROC has diplomatic relations , including the vice-president of El Salvador and the second vice president of Panama .\"]\n",
    "s2 = [\"On the 16th Chen attended the inauguration of President Hipolito Mejia .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n",
    "s1 = [\"The next day , Chen took advantage of the upcoming presidential inauguration to meet with leaders from a number of countries with which the ROC has diplomatic relations , including the vice-president of El Salvador and the second vice president of Panama .\"]\n",
    "s2 = [\"On the 16th the second vice president of Panama attended the inauguration of President Hipolito Mejia .\"]\n",
    "\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8209], grad_fn=<DivBackward0>)\n",
      "tensor([-2.2082], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s1 = [\"Costa Rica 's President Miguel Rodriguez also accompanied President Chen on a tour of a Taiwanese export processing zone , an orchid farm run by Taiwan Sugar Corporation , and a vocational training school .\"]\n",
    "s2 = [\"President Chen departed on the 20th for Africa , where the ROC has eight diplomatic partners .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n",
    "s1 = [\"Costa Rica 's President Miguel Rodriguez also accompanied President Chen on a tour of a Taiwanese export processing zone , an orchid farm run by Taiwan Sugar Corporation , and a vocational training school .\"]\n",
    "s2 = [\"President Rodriguez departed on the 20th for Africa , where the ROC has eight diplomatic partners .\"]\n",
    "\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1598], grad_fn=<DivBackward0>)\n",
      "tensor([1.1417], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s1 = [\"Later , he studied in France for five years , where he became well versed in the various schools of modern Western music .\"]\n",
    "s2 = [\"In 1959 he returned to introduce modern Western classical music to his homeland .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))\n",
    "\n",
    "s1 = [\"Later , he studied in France for five years , where he became well versed in the various schools of modern Western music .\"]\n",
    "s2 = [\"In 1959 France returned to introduce modern Western classical music to its homeland .\"]\n",
    "sents_encoded = one_the_fly_encoder(s1+s2)\n",
    "pos_sent1 = Variable(torch.from_numpy(np.array(sents_encoded[s1[0]])))\n",
    "pos_sent2 = Variable(torch.from_numpy(np.array(sents_encoded[s2[0]])))\n",
    "print(model.discriminator(pos_sent1, pos_sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coh",
   "language": "python",
   "name": "coh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
